{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "score_{new} = \\color{orange}{max(scores_{new})} - \\color{blue}{(max(scores_{new})-min(scores_{new}))} \\cdot \\color{red}{\\frac{max(scores_{old})-score_{old}}{max(scores_{old})-min(scores_{old})}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_old_min=4\n",
    "score_old_max=1\n",
    "score_new_min=10\n",
    "score_new_max=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "20  20\n",
      "16  16\n",
      "12  12\n",
      "10  10\n",
      "      0\n",
      "20  1.0\n",
      "16  2.2\n",
      "12  3.4\n",
      "10  4.0\n"
     ]
    }
   ],
   "source": [
    "def scores_scale(X, scores_old_min, scores_old_max, scores_new_min=0, scores_new_max=1):\n",
    "    X = scores_new_max - ((scores_new_max - scores_new_min) * (scores_old_max - X) / (scores_old_max - scores_old_min))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "0     \n",
      "20  20\n",
      "16  16\n",
      "12  12\n",
      "10  10\n",
      "      0\n",
      "0      \n",
      "20  1.0\n",
      "16  2.2\n",
      "12  3.4\n",
      "10  4.0\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame([[20, 16, 12, 10]]).T\n",
    "X.index = X[0]\n",
    "print(X)\n",
    "print(scores_scale(X=X, scores_old_min=10, scores_old_max=20, scores_new_min=4, scores_new_max=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforms features by scaling each feature to given scoring scale.\n",
    "\n",
    "    This estimator scales and translates each feature individually such\n",
    "    that it acccords with a given range on the training set, e.g. between\n",
    "    zero and ten.\n",
    "\n",
    "    The transformation is given by::\n",
    "\n",
    "        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "        X_scaled = X_std * (max - min) + min\n",
    "\n",
    "    where min, max = feature_range.\n",
    "\n",
    "    This transformation is often used as an alternative to zero mean,\n",
    "    unit variance scaling.\n",
    "\n",
    "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_range : tuple (min, max), default=(0, 1)\n",
    "        Desired range of transformed data.\n",
    "\n",
    "    copy : boolean, optional, default True\n",
    "        Set to False to perform inplace row normalization and avoid a\n",
    "        copy (if the input is already a numpy array).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    min_ : ndarray, shape (n_features,)\n",
    "        Per feature adjustment for minimum.\n",
    "\n",
    "    scale_ : ndarray, shape (n_features,)\n",
    "        Per feature relative scaling of the data.\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           *scale_* attribute.\n",
    "\n",
    "    data_min_ : ndarray, shape (n_features,)\n",
    "        Per feature minimum seen in the data\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           *data_min_*\n",
    "\n",
    "    data_max_ : ndarray, shape (n_features,)\n",
    "        Per feature maximum seen in the data\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           *data_max_*\n",
    "\n",
    "    data_range_ : ndarray, shape (n_features,)\n",
    "        Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
    "\n",
    "        .. versionadded:: 0.17\n",
    "           *data_range_*\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "    >>> scaler = MinMaxScaler()\n",
    "    >>> print(scaler.fit(data))\n",
    "    MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "    >>> print(scaler.data_max_)\n",
    "    [ 1. 18.]\n",
    "    >>> print(scaler.transform(data))\n",
    "    [[0.   0.  ]\n",
    "     [0.25 0.25]\n",
    "     [0.5  0.5 ]\n",
    "     [1.   1.  ]]\n",
    "    >>> print(scaler.transform([[2, 2]]))\n",
    "    [[1.5 0. ]]\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    minmax_scale: Equivalent function without the estimator API.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    NaNs are treated as missing values: disregarded in fit, and maintained in\n",
    "    transform.\n",
    "\n",
    "    For a comparison of the different scalers, transformers, and normalizers,\n",
    "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
    "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scores_old_min='auto', scores_old_max='auto', scores_new_min=0, scores_new_max=1, copy=True):\n",
    "        self.scores_old_min = scores_old_min\n",
    "        self.scores_old_max = scores_old_max\n",
    "        self.scores_new_min = scores_new_min\n",
    "        self.scores_new_max = scores_new_max\n",
    "        self.copy = copy\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset internal data-dependent state of the scaler, if necessary.\n",
    "\n",
    "        __init__ parameters are not touched.\n",
    "        \"\"\"\n",
    "\n",
    "        # Checking one attribute is enough, becase they are all set together\n",
    "        # in partial_fit\n",
    "        if hasattr(self, 'scale_'):\n",
    "            del self.scale_\n",
    "            del self.min_\n",
    "            del self.n_samples_seen_\n",
    "            del self.data_min_\n",
    "            del self.data_max_\n",
    "            del self.data_range_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the minimum and maximum to be used for later scaling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to compute the per-feature minimum and maximum\n",
    "            used for later scaling along the features axis.\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "        return self.partial_fit(X, y)\n",
    "\n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Online computation of min and max on X for later scaling.\n",
    "        All of X is processed as a single batch. This is intended for cases\n",
    "        when `fit` is not feasible due to very large number of `n_samples`\n",
    "        or because X is read from a continuous stream.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to compute the mean and standard deviation\n",
    "            used for later scaling along the features axis.\n",
    "\n",
    "        y\n",
    "            Ignored\n",
    "        \"\"\"\n",
    "        feature_range = self.feature_range\n",
    "        if feature_range[0] >= feature_range[1]:\n",
    "            raise ValueError(\"Minimum of desired feature range must be smaller\"\n",
    "                             \" than maximum. Got %s.\" % str(feature_range))\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            raise TypeError(\"MinMaxScaler does no support sparse input. \"\n",
    "                            \"You may consider to use MaxAbsScaler instead.\")\n",
    "\n",
    "        X = check_array(X, copy=self.copy, warn_on_dtype=True,\n",
    "                        estimator=self, dtype=FLOAT_DTYPES,\n",
    "                        force_all_finite=\"allow-nan\")\n",
    "\n",
    "        data_min = np.nanmin(X, axis=0)\n",
    "        data_max = np.nanmax(X, axis=0)\n",
    "\n",
    "        # First pass\n",
    "        if not hasattr(self, 'n_samples_seen_'):\n",
    "            self.n_samples_seen_ = X.shape[0]\n",
    "        # Next steps\n",
    "        else:\n",
    "            data_min = np.minimum(self.data_min_, data_min)\n",
    "            data_max = np.maximum(self.data_max_, data_max)\n",
    "            self.n_samples_seen_ += X.shape[0]\n",
    "\n",
    "        data_range = data_max - data_min\n",
    "        self.scale_ = ((feature_range[1] - feature_range[0]) /\n",
    "                       _handle_zeros_in_scale(data_range))\n",
    "        self.min_ = feature_range[0] - data_min * self.scale_\n",
    "        self.data_min_ = data_min\n",
    "        self.data_max_ = data_max\n",
    "        self.data_range_ = data_range\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Scaling features of X according to feature_range.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            Input data that will be transformed.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'scale_')\n",
    "\n",
    "        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n",
    "                        force_all_finite=\"allow-nan\")\n",
    "\n",
    "        X *= self.scale_\n",
    "        X += self.min_\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Undo the scaling of X according to feature_range.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            Input data that will be transformed. It cannot be sparse.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'scale_')\n",
    "\n",
    "        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n",
    "                        force_all_finite=\"allow-nan\")\n",
    "\n",
    "        X -= self.min_\n",
    "        X /= self.scale_\n",
    "        return X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
